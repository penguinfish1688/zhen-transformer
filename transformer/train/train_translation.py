"""
Training Script for Chinese-English Translation

This script demonstrates how to train the Transformer model
on Chinese-English translation data.

Usage:
    python -m transformer.train.train_translation
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import torch
import torch.nn as nn
from tqdm import tqdm
import time

from transformer.data.dataset import create_pipeline
from transformer.models.transformer import Transformer


def train_epoch(model, dataloader, optimizer, criterion, device, epoch, max_len):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    num_batches = 0
    
    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}")
    
    for src_batch, tgt_input, tgt_output in progress_bar:
        src_batch = src_batch.to(device)
        tgt_input = tgt_input.to(device)
        tgt_output = tgt_output.to(device)

        # Truncate long sentences
        src_len = src_batch.size(1)
        if src_len > max_len:
            src_batch = src_batch[:, :max_len]
        tgt_input_len = tgt_input.size(1)
        if tgt_input_len > max_len:
            tgt_input = tgt_input[:, :max_len]
            tgt_output = tgt_output[:, :max_len]

        # Forward pass (teacher forcing already applied in dataset)
        optimizer.zero_grad()
        logits = model(src_batch, tgt_input) # logits: (batch_size, tgt_len, tgt_vocab_size)

        # Compute loss
        logits_flat = logits.reshape(-1, logits.shape[-1]) # (batch_size * (tgt_len - 1), tgt_vocab_size)
        targets_flat = tgt_output.reshape(-1) # (batch_size * (tgt_len - 1))
        loss = criterion(logits_flat, targets_flat)
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
        
        progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
    
    return total_loss / num_batches


def evaluate(model, dataloader, criterion, device, max_len):
    """Evaluate on validation set"""
    model.eval()
    total_loss = 0
    num_batches = 0
    
    with torch.no_grad():
        for src_batch, tgt_input, tgt_output in dataloader:
            # Truncate long sentences
            src_len = src_batch.size(1)
            if src_len > max_len:
                src_batch = src_batch[:, :max_len]
            tgt_input_len = tgt_input.size(1)
            if tgt_input_len > max_len:
                tgt_input = tgt_input[:, :max_len]
                tgt_output = tgt_output[:, :max_len]

            src_batch = src_batch.to(device)
            tgt_input = tgt_input.to(device)
            tgt_output = tgt_output.to(device)

            logits = model(src_batch, tgt_input)
            
            logits_flat = logits.reshape(-1, logits.shape[-1])
            targets_flat = tgt_output.reshape(-1)
            loss = criterion(logits_flat, targets_flat)
            
            total_loss += loss.item()
            num_batches += 1
    
    return total_loss / max(num_batches, 1)


def greedy_decode(model, src_sentence, tokenizer, max_len, device="cpu"):
    """Decode using greedy search"""
    model.eval()
    
    # Tokenize and encode source
    src_tokens = tokenizer.tokenize_source(src_sentence)
    
    if len(src_tokens) > max_len:
        src_tokens = src_tokens[:max_len]

    src_ids = tokenizer.src_vocab.encode(src_tokens)
    src_tensor = torch.tensor([src_ids]).to(device)
    
    # Get encoder output
    with torch.no_grad():
        memory = model.encode(src_tensor)
    
    # Start decoding
    bos_idx = tokenizer.tgt_vocab.bos_idx
    eos_idx = tokenizer.tgt_vocab.eos_idx
    
    tgt_ids = [bos_idx]
    
    with torch.no_grad():
        for _ in range(max_len):
            tgt_tensor = torch.tensor([tgt_ids]).to(device)
            output = model.decode(tgt_tensor, memory, src_tensor)
            
            next_token_logits = output[0, -1, :]
            next_token = next_token_logits.argmax().item()
            
            tgt_ids.append(next_token)
            
            if next_token == eos_idx:
                break
    
    # Decode to text
    result_tokens = tokenizer.tgt_vocab.decode(tgt_ids)
    return " ".join(result_tokens)


def load_model_and_translate(checkpoint_path, config_path="transformer/config.yaml", 
                             test_sentences=None, use_sample=True):
    """
    Load a saved model checkpoint and run translation examples
    
    Args:
        checkpoint_path: Path to the checkpoint file (e.g., 'checkpoints/best_model.pt')
        config_path: Path to the YAML config file
        test_sentences: List of Chinese sentences to translate (default: predefined examples)
        use_sample: Whether to use sample data for loading vocabularies
    
    Returns:
        Tuple of (model, tokenizer)
    """
    print("\n" + "="*60)
    print("ðŸ“¥ LOADING MODEL AND RUNNING TRANSLATIONS")
    print("="*60)
    
    # Load config from YAML
    from transformer.data.config import TranslationConfig
    config = TranslationConfig.from_yaml(config_path)
    
    print(f"\nðŸ“‹ Configuration loaded from: {config_path}")
    print(f"   - Model: {config.num_layers} layers, {config.embed_dim} dim, {config.num_heads} heads")
    print(f"   - Max length: {config.max_len}")
    
    # Create pipeline to get tokenizer and vocabularies
    # We need this to load the vocabulary for encoding/decoding
    print(f"\nðŸ“š Loading vocabularies...")
    pipeline, tokenizer, _ = create_pipeline(use_sample=use_sample, config=config)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nðŸ“± Device: {device}")
    
    # Create model with same architecture
    model = Transformer(
        src_vocab_size=len(tokenizer.src_vocab),
        tgt_vocab_size=len(tokenizer.tgt_vocab),
        embed_dim=config.embed_dim,
        num_heads=config.num_heads,
        num_layers=config.num_layers,
        max_len=config.max_len,
        dropout=config.dropout,
        src_pad_idx=tokenizer.src_vocab.pad_idx,
        tgt_pad_idx=tokenizer.tgt_vocab.pad_idx
    ).to(device)
    
    # Load checkpoint
    print(f"\nðŸ’¾ Loading checkpoint from: {checkpoint_path}")
    if not os.path.exists(checkpoint_path):
        print(f"âŒ Error: Checkpoint file not found at {checkpoint_path}")
        return None, None
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # Load model state
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… Model loaded successfully!")
        print(f"   - Trained for {checkpoint.get('epoch', 'unknown')} epochs")
        print(f"   - Training loss: {checkpoint.get('train_loss', 'unknown'):.4f}")
        print(f"   - Validation loss: {checkpoint.get('val_loss', 'unknown'):.4f}")
    else:
        # Old format - just model state dict
        model.load_state_dict(checkpoint)
        print(f"âœ… Model loaded successfully (old format)")
    
    model.eval()
    
    # Default test sentences if none provided
    if test_sentences is None:
        test_sentences = [
            "ä½ å¥½",
            "è°¢è°¢ä½ ",
            "æˆ‘å–œæ¬¢å­¦ä¹ ä¸­æ–‡",
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
            "æˆ‘ä½åœ¨åŒ—äº¬",
            "è¿™æœ¬ä¹¦å¾ˆæœ‰è¶£",
            "æ˜Žå¤©è§",
            "ç¥ä½ å¥½è¿"
        ]
    
    # Run translations
    print("\n" + "="*60)
    print("ðŸ”® TRANSLATION EXAMPLES")
    print("="*60)
    
    for i, src in enumerate(test_sentences, 1):
        translation = greedy_decode(model, src, tokenizer, config.max_len, device=device)
        print(f"\n[{i}]")
        print(f"   ðŸ‡¨ðŸ‡³ Chinese: {src}")
        print(f"   ðŸ‡ºðŸ‡¸ English: {translation}")
    
    print("\n" + "="*60)
    print("âœ… Translation complete!")
    print("="*60)
    
    return model, tokenizer


def train(config_path="transformer/config.yaml", use_sample=True, resume_from=None, multi_gpu=False):
    """Main training function
    
    Args:
        config_path: Path to YAML config file
        use_sample: Whether to use sample dataset
        resume_from: Path to checkpoint to resume training from (optional)
        multi_gpu: Whether to use multiple GPUs with DataParallel (default: False)
    """
    
    print("\n" + "="*60)
    print("ðŸŽ“ CHINESE-ENGLISH TRANSLATION TRAINING")
    print("="*60)
    
    # Load config from YAML
    from transformer.data.config import TranslationConfig
    config = TranslationConfig.from_yaml(config_path)
    
    print(f"\nðŸ“‹ Configuration loaded from: {config_path}")
    print(f"   - Epochs: {config.num_epochs}")
    print(f"   - Batch size: {config.batch_size}")
    print(f"   - Learning rate: {config.learning_rate}")
    print(f"   - Checkpoint frequency: {config.checkpoint_frequency}")
    print(f"   - Use cached dataset: {config.use_cached_dataset}")
    print(f"   - Download new data: {config.download_new}")
    if resume_from:
        print(f"   - Resume from: {resume_from}")
    
    # Create pipeline
    pipeline, tokenizer, _ = create_pipeline(use_sample=use_sample, config=config)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nðŸ“± Device: {device}")
    
    # Create model
    model = Transformer(
        src_vocab_size=len(tokenizer.src_vocab),
        tgt_vocab_size=len(tokenizer.tgt_vocab),
        embed_dim=config.embed_dim,
        num_heads=config.num_heads,
        num_layers=config.num_layers,
        max_len=config.max_len,
        dropout=config.dropout,
        src_pad_idx=tokenizer.src_vocab.pad_idx,
        tgt_pad_idx=tokenizer.tgt_vocab.pad_idx
    ).to(device)
    
    # Multi-GPU setup
    gpu_count = torch.cuda.device_count()
    if multi_gpu and gpu_count > 1:
        print(f"\nðŸš€ Using {gpu_count} GPUs with DataParallel")
        model = nn.DataParallel(model)
        print(f"   âœ… Model wrapped with DataParallel")
        print(f"   ðŸ“Š GPU devices: {list(range(gpu_count))}")
    elif multi_gpu and gpu_count <= 1:
        print(f"\nâš ï¸  Multi-GPU requested but only {gpu_count} GPU(s) available")
        print(f"   Running on single device: {device}")
    
    # Count parameters
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ðŸ“Š Model parameters: {num_params:,}")
    
    # Optimizer and loss
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-9)
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.tgt_vocab.pad_idx)
    
    # Load checkpoint if resuming
    start_epoch = 1
    best_val_loss = float('inf')
    
    if resume_from:
        if os.path.exists(resume_from):
            print(f"\nðŸ“¥ Loading checkpoint from: {resume_from}")
            checkpoint = torch.load(resume_from, map_location=device)
            
            # Load model state
            if 'model_state_dict' in checkpoint:
                # Handle DataParallel state dict (with 'module.' prefix)
                state_dict = checkpoint['model_state_dict']
                if multi_gpu and not list(state_dict.keys())[0].startswith('module.'):
                    # Loading non-DataParallel checkpoint into DataParallel model
                    model.module.load_state_dict(state_dict)
                elif not multi_gpu and list(state_dict.keys())[0].startswith('module.'):
                    # Loading DataParallel checkpoint into non-DataParallel model
                    # Remove 'module.' prefix
                    from collections import OrderedDict
                    new_state_dict = OrderedDict()
                    for k, v in state_dict.items():
                        name = k[7:] if k.startswith('module.') else k  # remove 'module.' prefix
                        new_state_dict[name] = v
                    model.load_state_dict(new_state_dict)
                else:
                    model.load_state_dict(state_dict)
                print(f"   âœ… Model state loaded")
            else:
                model.load_state_dict(checkpoint)
                print(f"   âœ… Model state loaded (old format)")
            
            # Load optimizer state
            if 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                print(f"   âœ… Optimizer state loaded")
            
            # Get start epoch and best loss
            if 'epoch' in checkpoint:
                start_epoch = checkpoint['epoch'] + 1
                print(f"   âœ… Resuming from epoch {start_epoch}")
            
            if 'val_loss' in checkpoint:
                best_val_loss = checkpoint['val_loss']
                print(f"   âœ… Previous best val_loss: {best_val_loss:.4f}")
            
            print(f"   ðŸ“Š Previous training loss: {checkpoint.get('train_loss', 'N/A')}")
        else:
            print(f"\nâš ï¸  Warning: Checkpoint file not found at {resume_from}")
            print(f"   Starting training from scratch...")
    
    # DataLoaders
    train_loader = pipeline.get_dataloader("train", shuffle=True)
    val_loader = pipeline.get_dataloader("val", shuffle=False)
    
    # Training loop
    if resume_from and start_epoch > 1:
        print(f"\nðŸš€ Resuming training from epoch {start_epoch} to {config.num_epochs}...")
    else:
        print(f"\nðŸš€ Starting training for {config.num_epochs} epochs...")
    print(f"   Checkpoints will be saved every {config.checkpoint_frequency} epochs")
    print("-" * 60)
    
    for epoch in range(start_epoch, config.num_epochs + 1):
        start_time = time.time()
        
        # Train
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, epoch, config.max_len)
        
        # Validate
        val_loss = evaluate(model, val_loader, criterion, device, config.max_len)
        
        elapsed = time.time() - start_time
        
        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_path = os.path.join(config.checkpoint_dir, "best_model.pt")
            # Save model state (handle DataParallel wrapper)
            model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()
            torch.save({
                'epoch': epoch,
                'model_state_dict': model_state,
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'train_loss': train_loss,
            }, best_model_path)
            print(f"   ðŸ’¾ Saved best model (val_loss: {val_loss:.4f})")
        
        # Save checkpoint every N epochs
        if epoch % config.checkpoint_frequency == 0:
            checkpoint_path = os.path.join(config.checkpoint_dir, f"checkpoint_epoch_{epoch}.pt")
            # Save model state (handle DataParallel wrapper)
            model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()
            torch.save({
                'epoch': epoch,
                'model_state_dict': model_state,
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'train_loss': train_loss,
            }, checkpoint_path)
            print(f"   ðŸ’¾ Saved checkpoint at epoch {epoch}")
        
        print(f"   Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, time={elapsed:.1f}s")
    
    print("-" * 60)
    print(f"âœ… Training complete! Best val_loss: {best_val_loss:.4f}")
    
    # Test some translations
    print("\n" + "="*60)
    print("ðŸ”® SAMPLE TRANSLATIONS (untrained model produces random output)")
    print("="*60)
    
    test_sentences = ["ä½ å¥½", "è°¢è°¢ä½ ", "æˆ‘å–œæ¬¢å­¦ä¹ ä¸­æ–‡"]
    
    for src in test_sentences:
        translation = greedy_decode(model, src, tokenizer, config.max_len, device=device)
        print(f"   ðŸ‡¨ðŸ‡³ {src}")
        print(f"   ðŸ‡ºðŸ‡¸ {translation}")
        print()
    
    return model, tokenizer


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Train or test Chinese-English translation model')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'],
                        help='Mode: train a new model or test existing model')
    parser.add_argument('--checkpoint', type=str, default='checkpoints/best_model.pt',
                        help='Path to checkpoint file (for test mode or resume training)')
    parser.add_argument('--config', type=str, default='transformer/config.yaml',
                        help='Path to config YAML file')
    parser.add_argument('--use_sample', action='store_true',
                        help='Use sample data instead of full dataset')
    parser.add_argument('--resume', action='store_true',
                        help='Resume training from checkpoint (use with --checkpoint)')
    parser.add_argument('--multi_gpu', action='store_true',
                        help='Use multiple GPUs with DataParallel')
    parser.add_argument('--sentences', type=str, nargs='+',
                        help='Custom sentences to translate (for test mode)')
    
    args = parser.parse_args()
    
    if args.mode == 'train':
        # Train using YAML config
        resume_checkpoint = args.checkpoint if args.resume else None
        model, tokenizer = train(
            config_path=args.config,
            use_sample=args.use_sample,
            resume_from=resume_checkpoint,
            multi_gpu=args.multi_gpu
        )
    else:
        # Test mode - load and translate
        model, tokenizer = load_model_and_translate(
            checkpoint_path=args.checkpoint,
            config_path=args.config,
            test_sentences=args.sentences,
            use_sample=args.use_sample
        )
