"""
Dataset and DataLoader for Chinese-English Translation

This module provides:
1. Dataset download and preprocessing
2. PyTorch Dataset class
3. Collate function for batching with padding
4. DataLoader creation
"""
import os
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List, Tuple, Optional, Dict
import random


class TranslationDataset(Dataset):
    """
    PyTorch Dataset for parallel translation data
    """
    
    def __init__(self, src_data: List[List[int]], tgt_data: List[List[int]]):
        """
        Args:
            src_data: List of tokenized source sequences (as indices)
            tgt_data: List of tokenized target sequences (as indices)
        """
        assert len(src_data) == len(tgt_data), "Source and target must have same length"
        self.src_data = src_data
        self.tgt_data = tgt_data
    
    def __len__(self) -> int:
        return len(self.src_data)
    
    def __getitem__(self, idx: int) -> Tuple[List[int], List[int]]:
        return self.src_data[idx], self.tgt_data[idx]


def collate_fn(batch: List[Tuple[List[int], List[int]]], 
               src_pad_idx: int, tgt_pad_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Collate function to pad sequences in a batch
    
    Args:
        batch: List of (source, target) pairs
        src_pad_idx: Padding index for source
        tgt_pad_idx: Padding index for target
    
    Returns:
        Tuple of padded (source_batch, target_batch) tensors
    """
    src_batch, tgt_batch = zip(*batch)
    
    # Find max lengths
    src_max_len = max(len(s) for s in src_batch)
    tgt_max_len = max(len(t) for t in tgt_batch)
    
    # Pad sequences
    src_padded = []
    tgt_padded = []
    
    for src, tgt in zip(src_batch, tgt_batch):
        src_padded.append(src + [src_pad_idx] * (src_max_len - len(src)))
        tgt_padded.append(tgt + [tgt_pad_idx] * (tgt_max_len - len(tgt)))
    
    return torch.tensor(src_padded, dtype=torch.long), torch.tensor(tgt_padded, dtype=torch.long)


def download_sample_data() -> Tuple[List[str], List[str]]:
    """
    Provide sample Chinese-English parallel data for testing
    This is a small sample dataset. For production, use larger datasets.
    
    Returns:
        Tuple of (chinese_sentences, english_sentences)
    """
    # Sample parallel sentences for testing
    parallel_data = [
        ("ä½ å¥½", "hello"),
        ("æ—©ä¸Šå¥½", "good morning"),
        ("æ™šä¸Šå¥½", "good evening"),
        ("è°¢è°¢ä½ ", "thank you"),
        ("ä¸å®¢æ°”", "you are welcome"),
        ("æˆ‘æ˜¯å­¦ç”Ÿ", "i am a student"),
        ("ä»–æ˜¯è€å¸ˆ", "he is a teacher"),
        ("å¥¹å¾ˆæ¼‚äº®", "she is beautiful"),
        ("ä»Šå¤©å¤©æ°”å¾ˆå¥½", "the weather is nice today"),
        ("æˆ‘å–œæ¬¢å­¦ä¹ ä¸­æ–‡", "i like learning chinese"),
        ("è¿™æœ¬ä¹¦å¾ˆæœ‰è¶£", "this book is interesting"),
        ("æˆ‘ä»¬åŽ»åƒé¥­å§", "let us go eat"),
        ("ä½ å«ä»€ä¹ˆåå­—", "what is your name"),
        ("æˆ‘ä½åœ¨åŒ—äº¬", "i live in beijing"),
        ("ä¸­å›½æ˜¯ä¸€ä¸ªå¤§å›½", "china is a big country"),
        ("æˆ‘çˆ±æˆ‘çš„å®¶äºº", "i love my family"),
        ("æ˜Žå¤©è§", "see you tomorrow"),
        ("ç¥ä½ å¥½è¿", "good luck to you"),
        ("è¿™ä¸ªå¤šå°‘é’±", "how much is this"),
        ("è¯·é—®æ´—æ‰‹é—´åœ¨å“ªé‡Œ", "where is the restroom please"),
        ("æˆ‘ä¸æ˜Žç™½", "i do not understand"),
        ("ä½ èƒ½å¸®æˆ‘å—", "can you help me"),
        ("æˆ‘é¥¿äº†", "i am hungry"),
        ("æ°´åœ¨å“ªé‡Œ", "where is the water"),
        ("çŽ°åœ¨å‡ ç‚¹äº†", "what time is it now"),
        ("æˆ‘éœ€è¦ä¼‘æ¯", "i need to rest"),
        ("è¿™ä¸ªå¾ˆå¥½åƒ", "this is delicious"),
        ("æˆ‘å–œæ¬¢éŸ³ä¹", "i like music"),
        ("ä»–å–œæ¬¢è¿åŠ¨", "he likes sports"),
        ("å¥¹åœ¨çœ‹ä¹¦", "she is reading a book"),
        ("æˆ‘ä»¬æ˜¯æœ‹å‹", "we are friends"),
        ("è¯·å", "please sit down"),
        ("è¯·è¯´æ…¢ä¸€ç‚¹", "please speak slowly"),
        ("æˆ‘ä¼šè¯´ä¸€ç‚¹ä¸­æ–‡", "i can speak a little chinese"),
        ("ä½ ä¼šè¯´è‹±æ–‡å—", "can you speak english"),
        ("æˆ‘æ­£åœ¨å­¦ä¹ ", "i am studying"),
        ("è¿™æ˜¯æˆ‘çš„ä¹¦", "this is my book"),
        ("é‚£æ˜¯ä½ çš„ç¬”", "that is your pen"),
        ("ä»–ä»¬åœ¨å·¥ä½œ", "they are working"),
        ("æˆ‘æ˜¨å¤©åŽ»äº†å•†åº—", "i went to the store yesterday"),
        ("æˆ‘æ¯å¤©æ—©ä¸Šè·‘æ­¥", "i run every morning"),
        ("å¥¹åœ¨åŽ¨æˆ¿åšé¥­", "she is cooking in the kitchen"),
        ("æˆ‘çš„æœ‹å‹æ¥è‡ªç¾Žå›½", "my friend is from america"),
        ("æˆ‘æƒ³å–å’–å•¡", "i want to drink coffee"),
        ("ä½ å–œæ¬¢ä»€ä¹ˆé¢œè‰²", "what color do you like"),
        ("æˆ‘æœ€å–œæ¬¢è“è‰²", "my favorite is blue"),
        ("è¿™ä»¶è¡£æœå¤ªè´µäº†", "this clothes is too expensive"),
        ("ä½ ä»Šå¤©çœ‹èµ·æ¥å¾ˆå¼€å¿ƒ", "you look happy today"),
        ("æˆ‘ä¸‹å‘¨åŽ»æ—…è¡Œ", "i will travel next week"),
        ("ä»–åœ¨å¤§å­¦å­¦ä¹ è®¡ç®—æœº", "he studies computer at university"),
    ]
    
    chinese_sentences = [pair[0] for pair in parallel_data]
    english_sentences = [pair[1] for pair in parallel_data]
    
    return chinese_sentences, english_sentences


def download_wmt_sample(num_samples: int = 1000) -> Tuple[List[str], List[str]]:
    """
    Download a sample from common translation datasets.
    
    For a real application, you would use datasets like:
    - WMT (Workshop on Machine Translation)
    - UN Parallel Corpus
    - OpenSubtitles
    - OPUS (Open Parallel Corpus)
    
    This function provides instructions for downloading real datasets.
    """
    print("\n" + "="*60)
    print("ðŸ“¥ DATASET RECOMMENDATIONS FOR CHINESE-ENGLISH TRANSLATION")
    print("="*60)
    print("""
For production use, download one of these datasets:

1. **WMT News Translation** (Recommended for quality)
   - URL: https://www.statmt.org/wmt21/translation-task.html
   - Size: ~25M sentence pairs
   
2. **OPUS-100** (Easy to use with HuggingFace)
   - pip install datasets
   - from datasets import load_dataset
   - dataset = load_dataset("opus100", "en-zh")
   
3. **AI Challenger Translation Dataset**
   - URL: https://challenger.ai/
   - Size: 10M sentence pairs
   
4. **UN Parallel Corpus**
   - URL: https://conferences.unite.un.org/uncorpus
   - High-quality formal translations

For now, using built-in sample data for testing...
""")
    print("="*60)
    
    return download_sample_data()


class TranslationDataPipeline:
    """
    Complete data pipeline for Chinese-English translation
    """
    
    def __init__(self, tokenizer, config):
        """
        Args:
            tokenizer: TranslationTokenizer instance
            config: TranslationConfig instance
        """
        self.tokenizer = tokenizer
        self.config = config
        
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None
    
    def load_data(self, use_sample: bool = True) -> Tuple[List[str], List[str]]:
        """
        Load parallel corpus data
        
        Args:
            use_sample: If True, use sample data; otherwise download larger dataset
        """
        print("\n" + "="*60)
        print("ðŸ“‚ LOADING DATA")
        print("="*60)
        
        if use_sample:
            print("ðŸ“ Using sample dataset for testing...")
            src_sentences, tgt_sentences = download_sample_data()
        else:
            src_sentences, tgt_sentences = download_wmt_sample()
        
        print(f"âœ… Loaded {len(src_sentences)} parallel sentence pairs")
        print(f"\nðŸ“Š Sample pairs:")
        for i in range(min(3, len(src_sentences))):
            print(f"   [{i+1}] ZH: {src_sentences[i]}")
            print(f"       EN: {tgt_sentences[i]}")
        
        return src_sentences, tgt_sentences
    
    def prepare_data(self, src_sentences: List[str], tgt_sentences: List[str],
                    train_ratio: float = 0.8, val_ratio: float = 0.1) -> Dict:
        """
        Prepare data: tokenize, build vocab, split, and create datasets
        
        Args:
            src_sentences: Chinese sentences
            tgt_sentences: English sentences
            train_ratio: Ratio for training set
            val_ratio: Ratio for validation set
        
        Returns:
            Dictionary with datasets and info
        """
        print("\n" + "="*60)
        print("âš™ï¸  PREPARING DATA")
        print("="*60)
        
        # Step 1: Build vocabularies
        src_tokenized, tgt_tokenized = self.tokenizer.build_vocabularies(
            src_sentences, tgt_sentences, 
            min_freq=self.config.min_freq
        )
        
        # Step 2: Encode all sentences
        print("\nðŸ“Š Step 5: Encoding sentences to indices...")
        src_encoded = [self.tokenizer.src_vocab.encode(tokens) for tokens in src_tokenized]
        tgt_encoded = [self.tokenizer.tgt_vocab.encode(tokens) for tokens in tgt_tokenized]
        print(f"   âœ… Encoded {len(src_encoded)} sentence pairs")
        
        # Show sample encoding
        print(f"\n   ðŸ“Š Sample encoding:")
        print(f"   ZH tokens: {src_tokenized[0]}")
        print(f"   ZH indices: {src_encoded[0]}")
        print(f"   EN tokens: {tgt_tokenized[0]}")
        print(f"   EN indices: {tgt_encoded[0]}")
        
        # Step 3: Split data
        print(f"\nðŸ“Š Step 6: Splitting data (train={train_ratio}, val={val_ratio})...")
        n = len(src_encoded)
        indices = list(range(n))
        random.seed(42)
        random.shuffle(indices)
        
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        train_indices = indices[:train_end]
        val_indices = indices[train_end:val_end]
        test_indices = indices[val_end:]
        
        print(f"   âœ… Train: {len(train_indices)} samples")
        print(f"   âœ… Val: {len(val_indices)} samples")
        print(f"   âœ… Test: {len(test_indices)} samples")
        
        # Step 4: Create datasets
        print(f"\nðŸ“Š Step 7: Creating PyTorch datasets...")
        
        self.train_dataset = TranslationDataset(
            [src_encoded[i] for i in train_indices],
            [tgt_encoded[i] for i in train_indices]
        )
        self.val_dataset = TranslationDataset(
            [src_encoded[i] for i in val_indices],
            [tgt_encoded[i] for i in val_indices]
        )
        self.test_dataset = TranslationDataset(
            [src_encoded[i] for i in test_indices],
            [tgt_encoded[i] for i in test_indices]
        )
        
        print(f"   âœ… Datasets created successfully!")
        
        print("\n" + "="*60)
        print("âœ… DATA PREPARATION COMPLETE")
        print("="*60)
        
        return {
            "train_size": len(train_indices),
            "val_size": len(val_indices),
            "test_size": len(test_indices),
            "src_vocab_size": len(self.tokenizer.src_vocab),
            "tgt_vocab_size": len(self.tokenizer.tgt_vocab),
        }
    
    def get_dataloader(self, split: str = "train", shuffle: bool = True) -> DataLoader:
        """
        Get DataLoader for specified split
        
        Args:
            split: One of "train", "val", "test"
            shuffle: Whether to shuffle data
        
        Returns:
            PyTorch DataLoader
        """
        dataset_map = {
            "train": self.train_dataset,
            "val": self.val_dataset,
            "test": self.test_dataset
        }
        
        dataset = dataset_map.get(split)
        if dataset is None:
            raise ValueError(f"Unknown split: {split}. Use 'train', 'val', or 'test'")
        
        # Create collate function with vocab padding indices
        from functools import partial
        collate = partial(
            collate_fn,
            src_pad_idx=self.tokenizer.src_vocab.pad_idx,
            tgt_pad_idx=self.tokenizer.tgt_vocab.pad_idx
        )
        
        return DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=shuffle,
            collate_fn=collate,
            num_workers=0
        )
    
    def get_sample_batch(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get a sample batch for testing"""
        dataloader = self.get_dataloader("train", shuffle=False)
        src_batch, tgt_batch = next(iter(dataloader))
        return src_batch, tgt_batch


def create_pipeline(use_sample: bool = True):
    """
    Convenience function to create complete data pipeline
    
    Args:
        use_sample: Use sample data (True) or download full dataset (False)
    
    Returns:
        Tuple of (pipeline, tokenizer, config)
    """
    from transformer.data.config import TranslationConfig
    from transformer.data.tokenizer import TranslationTokenizer
    
    print("\n" + "ðŸš€"*30)
    print("  CHINESE-ENGLISH TRANSLATION DATA PIPELINE")
    print("ðŸš€"*30)
    
    # Create config and tokenizer
    config = TranslationConfig()
    tokenizer = TranslationTokenizer(config)
    
    # Create pipeline
    pipeline = TranslationDataPipeline(tokenizer, config)
    
    # Load and prepare data
    src_sentences, tgt_sentences = pipeline.load_data(use_sample=use_sample)
    info = pipeline.prepare_data(src_sentences, tgt_sentences)
    
    print("\n" + "="*60)
    print("ðŸ“‹ PIPELINE SUMMARY")
    print("="*60)
    print(f"   Source vocabulary: {info['src_vocab_size']} tokens")
    print(f"   Target vocabulary: {info['tgt_vocab_size']} tokens")
    print(f"   Training samples: {info['train_size']}")
    print(f"   Validation samples: {info['val_size']}")
    print(f"   Test samples: {info['test_size']}")
    print(f"   Batch size: {config.batch_size}")
    print(f"   Device: {config.device}")
    print("="*60)
    
    return pipeline, tokenizer, config


if __name__ == "__main__":
    # Run pipeline demo
    pipeline, tokenizer, config = create_pipeline(use_sample=True)
    
    # Get sample batch
    print("\nðŸ“¦ Getting sample batch...")
    src_batch, tgt_batch = pipeline.get_sample_batch()
    print(f"   Source batch shape: {src_batch.shape}")
    print(f"   Target batch shape: {tgt_batch.shape}")
    print(f"   Source sample: {src_batch[0].tolist()}")
    print(f"   Target sample: {tgt_batch[0].tolist()}")
