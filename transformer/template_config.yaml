# Configuration for Chinese-English Translation
# Model Architecture
model:
  embed_dim: 512
  num_heads: 8
  num_layers: 8
  max_len: 512
  dropout: 0.1

# Training Parameters
training:
  batch_size: 32
  learning_rate: 0.0001
  num_epochs: 100
  warmup_steps: 4000
  checkpoint_frequency: 1  # Save checkpoint every N epochs

# Data Configuration
data:
  min_freq: 2  # Minimum word frequency to include in vocabulary
  max_samples: 1000000  # Limit samples for testing
  use_cached_dataset: false  # If true, load from cached dataset; if false, download new
  cache_dir: "data/translation/cache"  # Directory to save/load cached datasets
  download_new: false  # If true, always download new data from online

# Tokenization
tokenization:
  src_lang: "zh"  # Chinese
  tgt_lang: "en"  # English
  pad_token: "<pad>"
  unk_token: "<unk>"
  bos_token: "<bos>"  # Beginning of sentence
  eos_token: "<eos>"  # End of sentence

# Paths
paths:
  data_dir: "data/translation"
  checkpoint_dir: "/workspaces/bertsi/checkpoints"
  cache_dir: "data/translation/cache"
